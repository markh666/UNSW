{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import spacy\n",
    "import math\n",
    "import copy\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "## Read the data sets...\n",
    "\n",
    "### Read the Training Data\n",
    "# train_file = './Data/train.pickle'\n",
    "train_file = './Dev_set_2/train.pickle'\n",
    "train_mentions = pickle.load(open(train_file, 'rb'))\n",
    "\n",
    "### Read the Training Labels...\n",
    "# train_label_file = './Data/train_labels.pickle'\n",
    "train_label_file = './Dev_set_2/train_labels.pickle'\n",
    "train_labels = pickle.load(open(train_label_file, 'rb'))\n",
    "\n",
    "### Read the Dev Data... (For Final Evaluation, we will replace it with the Test Data)\n",
    "# dev_file = './Data/dev.pickle'\n",
    "dev_file = './Dev_set_2/dev2.pickle'\n",
    "dev_mentions = pickle.load(open(dev_file, 'rb'))\n",
    "\n",
    "### Read the Parsed Entity Candidate Pages...\n",
    "# fname = './Data/parsed_candidate_entities.pickle'\n",
    "fname = './Dev_set_2/parsed_candidate_entities.pickle'\n",
    "parsed_entity_pages = pickle.load(open(fname, 'rb'))\n",
    "\n",
    "### Read the Mention docs...\n",
    "# mens_docs_file = \"./Data/men_docs.pickle\"\n",
    "mens_docs_file = \"./Dev_set_2/men_docs.pickle\"\n",
    "men_docs = pickle.load(open(mens_docs_file, 'rb'))\n",
    "\n",
    "### Read the Dev Labels... (For Final Evaluation, we will replace it with the Test Data)\n",
    "# dev_label_file = './Data/dev_labels.pickle'\n",
    "dev_label_file = './Dev_set_2/dev2_labels.pickle'\n",
    "dev_labels = pickle.load(open(dev_label_file, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6871"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initiation(documents):\n",
    "    global tf_entities, tf_tokens, idf_entities, idf_tokens\n",
    "\n",
    "    tf_entities = {}\n",
    "    tf_tokens = {}\n",
    "    idf_tokens = {}\n",
    "    idf_entities = {}\n",
    "\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    ents_counter = {} # 用于和token中的entity抵消\n",
    "    for i in documents:\n",
    "        doc = nlp(documents[i])\n",
    "        for n in doc.ents:\n",
    "            if len(n) == 1:\n",
    "                for tmp in n:\n",
    "                    e = n.lemma_\n",
    "            else:\n",
    "                e = n.orth_\n",
    "            if e not in tf_entities:\n",
    "                tf_entities[e] = {}\n",
    "                tf_entities[e][i] = 1\n",
    "                if len(n) == 1:\n",
    "                    ents_counter[e] = {}\n",
    "                    ents_counter[e][i] = 1\n",
    "            elif i not in tf_entities[e]:\n",
    "                tf_entities[e][i] = 1\n",
    "                if len(n) == 1:\n",
    "                    ents_counter[e][i] = 1\n",
    "            else:\n",
    "                tf_entities[e][i] += 1\n",
    "                if len(n) == 1:\n",
    "                    ents_counter[e][i] += 1\n",
    "\n",
    "        for text in doc:\n",
    "            if not text.is_punct and not text.is_space and not text.is_stop:\n",
    "                word = text.lemma_\n",
    "                if word not in ents_counter or i not in ents_counter[word]:\n",
    "                    if word not in tf_tokens:\n",
    "                        tf_tokens[word] = {}\n",
    "                        tf_tokens[word][i] = 1\n",
    "                    elif i not in tf_tokens[word]:\n",
    "                        tf_tokens[word][i] = 1\n",
    "                    else:\n",
    "                        tf_tokens[word][i] += 1\n",
    "                else:\n",
    "                    if ents_counter[word][i] > 1:\n",
    "                        ents_counter[word][i] -= 1\n",
    "                    else:\n",
    "                        del ents_counter[word][i]\n",
    "\n",
    "    idf_tokens = copy.deepcopy(tf_tokens)\n",
    "    idf_entities = copy.deepcopy(tf_entities)\n",
    "\n",
    "    # update the score\n",
    "    for key in idf_tokens:\n",
    "        for i in idf_tokens[key]:\n",
    "            idf_tokens[key][i] = (1+math.log(1+math.log(idf_tokens[key][i]))) * (1+math.log(len(documents)/(1+len(idf_tokens[key]))))\n",
    "    for key in idf_entities:\n",
    "        for i in idf_entities[key]:\n",
    "            idf_entities[key][i] = (1+math.log(idf_entities[key][i])) * (1+math.log(len(documents)/(1+len(idf_entities[key]))))\n",
    "    \n",
    "    return tf_entities, tf_tokens, idf_entities, idf_tokens\n",
    "\n",
    "def tf_idf(candidate, doc_id):\n",
    "    tokens_score = entities_score = 0\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    candidate = nlp(candidate.replace('_', ' '))\n",
    "    for ent in candidate.ents:\n",
    "        if ent.lemma_ in idf_entities and doc_id in idf_entities[ent.lemma_]:\n",
    "            entities_score += idf_entities[ent.lemma_][doc_id]\n",
    "\n",
    "    for token in candidate:\n",
    "        if token.lemma_ in idf_tokens and doc_id in idf_tokens[token.lemma_]:\n",
    "            tokens_score += idf_tokens[token.lemma_][doc_id]\n",
    "\n",
    "    combined_score = (entities_score + tokens_score * 0.4)/len(candidate)\n",
    "\n",
    "    return combined_score\n",
    "\n",
    "def minDistance(word1, word2):\n",
    "    if not word1:\n",
    "        return len(word2 or '') or 0\n",
    "\n",
    "    if not word2:\n",
    "        return len(word1 or '') or 0\n",
    "\n",
    "    size1 = len(word1)\n",
    "    size2 = len(word2)\n",
    "\n",
    "    last = 0\n",
    "    tmp = list(range(size2 + 1))\n",
    "    value = None\n",
    "\n",
    "    for i in list(range(size1)):\n",
    "        tmp[0] = i + 1\n",
    "        last = i\n",
    "        for j in range(size2):\n",
    "            if word1[i] == word2[j]:\n",
    "                value = last\n",
    "            else:\n",
    "                value = 1 + min(last, tmp[j], tmp[j + 1])\n",
    "            last = tmp[j+1]\n",
    "            tmp[j+1] = value\n",
    "    return value\n",
    "\n",
    "def cosine(str1, str2):\n",
    "    str2 = str2.replace('_', ' ')\n",
    "    list_word1 = str1.split()\n",
    "#     print(list_word1)\n",
    "    list_word2 = str2.split()\n",
    "#     print(list_word2)\n",
    "    key_word = list(set(list_word1 + list_word2))\n",
    "    word_vector1 = np.zeros(len(key_word))\n",
    "    word_vector2 = np.zeros(len(key_word))\n",
    "    count = 0\n",
    "    flag = True\n",
    "    for i in range(len(key_word)):\n",
    "        for j in range(len(list_word1)):\n",
    "            if key_word[i] == list_word1[j]:\n",
    "                word_vector1[i] += 1\n",
    "     \n",
    "        for k in range(len(list_word2)):\n",
    "            if key_word[i] == list_word2[k]:\n",
    "                word_vector2[i] += 1\n",
    "            if list_word2[k] in list_word1 and flag:\n",
    "                count += 1  \n",
    "        flag = False\n",
    "\n",
    "    dist1=float(np.dot(word_vector1,word_vector2)/(np.linalg.norm(word_vector1)*np.linalg.norm(word_vector2)))\n",
    "    count = count/len(list_word1)\n",
    "    return dist1,count\n",
    "\n",
    "def label_value(data, label):\n",
    "    lis = []\n",
    "    for i in data:\n",
    "        for word in data[i]['candidate_entities']:\n",
    "            if word == label[i]['label']:\n",
    "                lis.append(1)\n",
    "            else:\n",
    "                lis.append(0)\n",
    "    return np.array(lis)\n",
    "\n",
    "def extract_features(mentions):\n",
    "    distance_list = []\n",
    "    tfidf_list = []\n",
    "    same_counter = []\n",
    "    cosine_list = []\n",
    "    for i in mentions:\n",
    "        for candidate in mentions[i]['candidate_entities']:\n",
    "            distance_list.append(minDistance(mentions[i]['mention'], candidate))\n",
    "            tfidf_list.append(tf_idf(candidate,mentions[i]['doc_title']))\n",
    "            cosine_value, same_count = cosine(mentions[i]['mention'], candidate)\n",
    "            cosine_list.append(cosine_value)\n",
    "            same_counter.append(same_count)\n",
    "    return np.array([tfidf_list, distance_list, cosine_list, same_counter]).T \n",
    "\n",
    "def data_group(mentions):\n",
    "    data_group = []\n",
    "    for i in mentions:\n",
    "        data_group.append(len(mentions[i]['candidate_entities']))\n",
    "    data_groups = np.array(data_group)\n",
    "    return data_groups\n",
    "\n",
    "def transform_data(features, groups, labels=None):\n",
    "    xgb_data = xgb.DMatrix(data=features, label=labels)\n",
    "    xgb_data.set_group(groups)\n",
    "    return xgb_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(result, data_labels):\n",
    "    assert set(list(result.keys())) - set(list(data_labels.keys())) == set()\n",
    "    TP = 0.0\n",
    "    for id_ in result.keys():\n",
    "        if result[id_] == data_labels[id_]['label']:\n",
    "            TP +=1\n",
    "    assert len(result) == len(data_labels)\n",
    "    return TP/len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disambiguate_mentions(train_mentions, train_labels, dev_mentions, men_docs, parsed_entity_pages):\n",
    "    start = time.time()\n",
    "    initiation(men_docs)\n",
    "    print(\"initiation cost time: {}\".format(time.time()-start))\n",
    "    \n",
    "    start = time.time()\n",
    "    train_data = extract_features(train_mentions)\n",
    "    train_groups = data_group(train_mentions)\n",
    "    train_label = label_value(train_mentions,train_labels)\n",
    "    xgboost_train = transform_data(train_data, train_groups, train_label)\n",
    "    print(\"train data set cost time: {}\".format(time.time()-start))\n",
    "    \n",
    "    # Test Data\n",
    "    start = time.time()\n",
    "    test_data = extract_features(dev_mentions)\n",
    "    test_groups = data_group(dev_mentions)\n",
    "    xgboost_test = transform_data(test_data, test_groups)\n",
    "    print(\"test data set cost time: {}\".format(time.time()-start))\n",
    "    \n",
    "    start = time.time()\n",
    "    param = {'max_depth': 8, 'eta': 0.05, 'silent': 1, 'objective': 'rank:pairwise',\n",
    "             'min_child_weight': 0.01, 'lambda':100}\n",
    "    classifier = xgb.train(param, xgboost_train, num_boost_round=4900)\n",
    "    ##  Predict test data...\n",
    "    preds = classifier.predict(xgboost_test)\n",
    "    print(\"predict cost time: {}\".format(time.time()-start))\n",
    "    \n",
    "    start = time.time()\n",
    "    result = []\n",
    "    counter = 0\n",
    "    preds = list(preds)\n",
    "    test_group = list(test_groups)\n",
    "\n",
    "    for i in range(len(test_group)):\n",
    "        tmp = preds[counter:(test_group[i]+counter)]\n",
    "        result.append(tmp.index(max(tmp)))\n",
    "        counter += test_group[i]\n",
    "\n",
    "    result_dict = {}\n",
    "    for i in range(len(result)):\n",
    "        result_dict[i+1] = dev_mentions[i+1]['candidate_entities'][result[i]]\n",
    "    print(\"build result cost time: {}\".format(time.time()-start))\n",
    "    \n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import project_part2 as project_part2\n",
    "## Read the data sets...\n",
    "\n",
    "### Read the Training Data\n",
    "train_file = './Data/train.pickle'\n",
    "train_mentions = pickle.load(open(train_file, 'rb'))\n",
    "\n",
    "### Read the Training Labels...\n",
    "train_label_file = './Data/train_labels.pickle'\n",
    "train_labels = pickle.load(open(train_label_file, 'rb'))\n",
    "\n",
    "### Read the Dev Data... (For Final Evaluation, we will replace it with the Test Data)\n",
    "dev_file = './Data/dev.pickle'\n",
    "dev_mentions = pickle.load(open(dev_file, 'rb'))\n",
    "\n",
    "### Read the Parsed Entity Candidate Pages...\n",
    "fname = './Data/parsed_candidate_entities.pickle'\n",
    "parsed_entity_pages = pickle.load(open(fname, 'rb'))\n",
    "\n",
    "### Read the Mention docs...\n",
    "mens_docs_file = \"./Data/men_docs.pickle\"\n",
    "men_docs = pickle.load(open(mens_docs_file, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initiation cost time: 3.5503878593444824\n",
      "train data set cost time: 46.79745531082153\n",
      "test data set cost time: 14.394601583480835\n",
      "predict cost time: 15.337562561035156\n",
      "build result cost time: 0.0\n"
     ]
    }
   ],
   "source": [
    "result = project_part2.disambiguate_mentions(train_mentions, train_labels, dev_mentions, men_docs, parsed_entity_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
