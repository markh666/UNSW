{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pickle\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = {1: 'According to Times of India, President Donald Trump was on his way to New York City after his address at UNGA.',\n",
    "             2: 'The New York Times mentioned an interesting story about Trump.',\n",
    "             3: 'I think it would be great if I can travel to New York this summer to see Trump.'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = 'The New New York City Times of India'\n",
    "DoE = {'Times of India':0, 'The New York Times':1,'New York City':2}\n",
    "doc_id = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_tokens = {}\n",
    "tf_entities = {}\n",
    "idf_tokens = {}\n",
    "idf_entities = {}\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "nlp = spacy.load(r'C:\\Users\\hanyu\\Anaconda3\\Lib\\site-packages\\en_core_web_sm\\en_core_web_sm-2.0.0')\n",
    "\n",
    "ents_counter = {} # 用于和token中的entity抵消\n",
    "for i in range(1, len(documents)+1):\n",
    "    doc = nlp(documents[i])\n",
    "    for n in doc.ents:\n",
    "        e = n.orth_\n",
    "        if e not in tf_entities:\n",
    "            tf_entities[e] = {}\n",
    "            tf_entities[e][i] = 1\n",
    "            if len(n) == 1:\n",
    "                ents_counter[e] = 1\n",
    "        elif i not in tf_entities[e]:\n",
    "            tf_entities[e][i] = 1\n",
    "        else:\n",
    "            tf_entities[e][i] += 1\n",
    "            if len(n) == 1:\n",
    "                ents_counter[e] += 1\n",
    "            \n",
    "    for text in doc:\n",
    "        if not text.is_punct and not text.is_space and not text.is_stop:\n",
    "            word = text.orth_\n",
    "            if word not in ents_counter:\n",
    "                if word not in tf_tokens:\n",
    "                    tf_tokens[word] = {}\n",
    "                    tf_tokens[word][i] = 1\n",
    "                elif i not in tf_tokens[word]:\n",
    "                    tf_tokens[word][i] = 1\n",
    "                else:\n",
    "                    tf_tokens[word][i] += 1\n",
    "            else:\n",
    "                if ents_counter[word] > 1:\n",
    "                    ents_counter[word] -= 1\n",
    "                else:\n",
    "                    del ents_counter[word]\n",
    "                    \n",
    "idf_tokens = copy.deepcopy(tf_tokens)\n",
    "idf_entities = copy.deepcopy(tf_entities)\n",
    "\n",
    "# update the score\n",
    "for key in idf_tokens:\n",
    "    for i in idf_tokens[key]:\n",
    "        idf_tokens[key][i] = (1+math.log(idf_tokens[key][i])) * (1+math.log(len(documents)/(1+len(idf_tokens[key]))))\n",
    "for key in idf_entities:\n",
    "    for i in idf_entities[key]:\n",
    "        idf_entities[key][i] = (1+math.log(idf_entities[key][i])) * (1+math.log(len(documents)/(1+len(idf_entities[key]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSubLists(lis=[],l=0):\n",
    "    res = []\n",
    "    for m in range(1, l+1):\n",
    "        allAns = []\n",
    "        ans = [None for i in range(m)]  \n",
    "        subLists(lis,m,ans,allAns)\n",
    "        res.extend(allAns)\n",
    "    return res\n",
    "\n",
    "def subLists(lis=[],m=0,ans=[],allAns=[]):\n",
    "    if m==0:\n",
    "        allAns.append(ans.copy()) \n",
    "        return\n",
    "    if len(lis)<m:\n",
    "        return\n",
    "    length=len(lis)\n",
    "    for iter in range(length-m+1):\n",
    "        ans[-m]=lis[iter]\n",
    "        if iter+1<length:\n",
    "            subLists(lis[iter+1:],m-1,ans,allAns)\n",
    "        else:\n",
    "            allAns.append(ans.copy())\n",
    "            return\n",
    "\n",
    "def Query_counter(Q_nlp):\n",
    "    q_count = {}\n",
    "    for word in Q_nlp:\n",
    "        word = word.orth_\n",
    "        if word not in q_count:\n",
    "            q_count[word] = 1\n",
    "        else:\n",
    "            q_count[word] += 1\n",
    "    return q_count\n",
    "\n",
    "def filter_options(all_ops, q_count):\n",
    "    after_filter = []\n",
    "    for op in all_ops:\n",
    "        dic_op = {}\n",
    "        flag = True\n",
    "        for item in op:\n",
    "            item = item.split()\n",
    "            for i in item:\n",
    "                if i not in dic_op:\n",
    "                    dic_op[i] = 1\n",
    "                else:\n",
    "                    dic_op[i] += 1\n",
    "        for word in dic_op:\n",
    "            if word not in q_count or q_count[word] < dic_op[word]:\n",
    "                flag = False\n",
    "                break\n",
    "        if flag:\n",
    "            after_filter.append(op)\n",
    "    return after_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_count = Query_counter(nlp(Q))\n",
    "Query_entity = []\n",
    "for i in DoE:\n",
    "    tmp = copy.deepcopy(q_count)\n",
    "    Flag = True\n",
    "    for j in nlp(i):\n",
    "        j = j.orth_\n",
    "        if j not in tmp or tmp[j] == 0:\n",
    "            Flag = False\n",
    "            break\n",
    "        else:\n",
    "            tmp[j] -= 1\n",
    "    if Flag:\n",
    "        Query_entity.append(i)\n",
    "all_ops = getSubLists(Query_entity, len(Query_entity))\n",
    "e_options = filter_options(all_ops, q_count)\n",
    "candidates = [i.orth_ for i in nlp(Q)]\n",
    "result = [{'tokens': candidates, 'entities': []}]\n",
    "for i in range(len(e_options)):\n",
    "    k = candidates.copy()\n",
    "    e = e_options[i]\n",
    "    e_tmp = []\n",
    "    for j in e:\n",
    "        e_tmp.extend(j.split())\n",
    "    for word in e_tmp:\n",
    "        k.remove(word)\n",
    "    result.append({'tokens': k, 'entities': e})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_score(result, ents, token, doc_id):\n",
    "    score_list = []\n",
    "    max_score = 0\n",
    "    index_list = []\n",
    "    for q in range(len(result)):\n",
    "        e, k, tokens_score, entities_score = result[q][0], result[q][1], 0, 0\n",
    "        for i in e:\n",
    "            if i in ents and doc_id  in ents[i]:\n",
    "                entities_score += ents[i][doc_id]\n",
    "        for j in k:\n",
    "            if j in token and doc_id in token[j]:\n",
    "                tokens_score += token[j][doc_id]\n",
    "        combined_score = entities_score + tokens_score * 0.4\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['The', 'New', 'New', 'York', 'City', 'Times', 'of', 'India'], 'entities': []}\n",
      "{'tokens_score': 5.947883998860986, 'entities_score': 0, 'combined_score': 2.3791535995443946}\n",
      "\n",
      "{'tokens': ['The', 'New', 'New', 'York', 'City'], 'entities': ['Times of India']}\n",
      "{'tokens_score': 3.5424188907528213, 'entities_score': 1.4054651081081644, 'combined_score': 2.822432664409293}\n",
      "\n",
      "{'tokens': ['New', 'City', 'of', 'India'], 'entities': ['The New York Times']}\n",
      "{'tokens_score': 3.5232481437645475, 'entities_score': 0, 'combined_score': 1.4092992575058192}\n",
      "\n",
      "{'tokens': ['The', 'New', 'Times', 'of', 'India'], 'entities': ['New York City']}\n",
      "{'tokens_score': 3.117783035656384, 'entities_score': 1.4054651081081644, 'combined_score': 2.652578322370718}\n",
      "\n",
      "{'tokens': ['The', 'New'], 'entities': ['Times of India', 'New York City']}\n",
      "{'tokens_score': 0.7123179275482191, 'entities_score': 2.8109302162163288, 'combined_score': 3.0958573872356165}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in result:\n",
    "    k, e, tokens_score, entities_score = i['tokens'], i['entities'], 0, 0\n",
    "    for h in k:\n",
    "        if h in idf_tokens and doc_id in idf_tokens[h]:\n",
    "            tokens_score += idf_tokens[h][doc_id]\n",
    "    for j in e:\n",
    "        if j in idf_entities and doc_id in idf_entities[j]:\n",
    "            entities_score += idf_entities[j][doc_id]\n",
    "    combined_score = entities_score + tokens_score * 0.4\n",
    "    print(i)\n",
    "    print({'tokens_score': tokens_score, 'entities_score': entities_score, 'combined_score': combined_score})\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "query =  {'tokens': ['The', 'New', 'New', 'York', 'City', 'Times', 'of', 'India'], 'entities': []}\n",
    "{'tokens_score': 5.947883998860986, 'entities_score': 0.0, 'combined_score': 2.3791535995443946}\n",
    "\n",
    "query =  {'tokens': ['The', 'New', 'Times', 'of', 'India'], 'entities': ['New York City']}\n",
    "{'tokens_score': 3.117783035656384, 'entities_score': 1.4054651081081644, 'combined_score': 2.652578322370718}\n",
    "\n",
    "query =  {'tokens': ['New', 'City', 'of', 'India'], 'entities': ['The New York Times']}\n",
    "{'tokens_score': 3.5232481437645475, 'entities_score': 0.0, 'combined_score': 1.4092992575058192}\n",
    "\n",
    "query =  {'tokens': ['The', 'New', 'New', 'York', 'City'], 'entities': ['Times of India']}\n",
    "{'tokens_score': 3.5424188907528213, 'entities_score': 1.4054651081081644, 'combined_score': 2.822432664409293}\n",
    "\n",
    "query =  {'tokens': ['The', 'New'], 'entities': ['New York City', 'Times of India']}\n",
    "{'tokens_score': 0.7123179275482191, 'entities_score': 2.8109302162163288, 'combined_score': 3.0958573872356165}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
